% Отчёт по НИР — Федеративное обучение рекомендательной системы
% НИТУ «МИСиС», кафедра инженерной кибернетики
% 7-й семестр, 2025–2026 уч. год

\documentclass[14pt,a4paper]{extarticle}

% ---- Кодировка и язык ----
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}

% ---- Шрифт Times New Roman ----
\usepackage{tempora}          % свободный аналог Times New Roman
\usepackage[scaled=0.85]{beramono}

% ---- Геометрия страницы: левое 30, правое 15, верх/низ 20 ----
\usepackage[left=30mm, right=15mm, top=20mm, bottom=20mm]{geometry}

% ---- Полуторный интервал ----
\usepackage{setspace}
\onehalfspacing

% ---- Абзацный отступ 1.25 см ----
\setlength{\parindent}{1.25cm}

% ---- Нумерация страниц: внизу по центру ----
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt}

% ---- Математика ----
\usepackage{amsmath,amssymb,amsfonts}

% ---- Графика ----
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{float}

% ---- Таблицы ----
\usepackage{booktabs}
\usepackage{array}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{tabularx}

% ---- Листинги кода ----
\usepackage{listings}
\lstset{
  basicstyle=\small\ttfamily,
  breaklines=true,
  frame=single,
  language=Python,
  numbers=left,
  numberstyle=\tiny,
  tabsize=4,
  inputencoding=utf8,
  extendedchars=true,
  keepspaces=true,
  columns=flexible,
  showstringspaces=false,
}

% ---- Гиперссылки ----
\usepackage[hidelinks]{hyperref}

% ---- Подписи к рисункам и таблицам ----
\usepackage[labelsep=endash]{caption}
\captionsetup[figure]{name=Рисунок}
\captionsetup[table]{name=Таблица, justification=raggedright, singlelinecheck=false}

% ---- Оформление заголовков разделов ----
\usepackage{titlesec}
\titleformat{\section}
  {\normalfont\bfseries\fontsize{14}{16}\selectfont}
  {\thesection}{0.5em}{}
\titleformat{\subsection}
  {\normalfont\bfseries\fontsize{14}{16}\selectfont}
  {\thesubsection}{0.5em}{}
\titleformat{\subsubsection}
  {\normalfont\bfseries\fontsize{14}{16}\selectfont}
  {\thesubsubsection}{0.5em}{}

\titlespacing*{\section}{1.25cm}{1em}{0.5em}
\titlespacing*{\subsection}{1.25cm}{0.8em}{0.4em}
\titlespacing*{\subsubsection}{1.25cm}{0.6em}{0.3em}

% Команда для ненумерованных структурных элементов (ВВЕДЕНИЕ, ВЫВОДЫ и т.д.) — по центру
\newcommand{\structsection}[1]{%
  \clearpage
  \phantomsection
  {\centering\bfseries\fontsize{14}{16}\selectfont #1\par}
  \vspace{0.5em}%
}

% ---- Содержание ----
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\renewcommand{\contentsname}{СОДЕРЖАНИЕ}

% ---- Библиография ----
\usepackage[numbers,sort&compress]{natbib}

% ---- Перечисления ----
\usepackage{enumitem}
\setlist{noitemsep, topsep=0pt}

% ---- Для схем ----
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, fit, calc}


\begin{document}

% Титульный лист пропущен — будет добавлен отдельно.
% Нумерация начинается со 2-й страницы.
\setcounter{page}{2}

% СОДЕРЖАНИЕ
\tableofcontents
\newpage


% СПИСОК ИСПОЛЬЗУЕМЫХ ОСНОВНЫХ СОКРАЩЕНИЙ
\structsection{СПИСОК ИСПОЛЬЗУЕМЫХ ОСНОВНЫХ СОКРАЩЕНИЙ}
\addcontentsline{toc}{section}{СПИСОК ИСПОЛЬЗУЕМЫХ ОСНОВНЫХ СОКРАЩЕНИЙ}

\noindent
\begin{tabularx}{\textwidth}{@{}l@{\hspace{1em}--\hspace{1em}}X@{}}
ВКР   & выпускная квалификационная работа \\
ИИ    & искусственный интеллект \\
МО    & машинное обучение \\
НИР   & научно-исследовательская работа \\
ФО    & федеративное обучение \\
NCF   & Neural Collaborative Filtering (нейронная коллаборативная фильтрация) \\
GMF   & Generalized Matrix Factorization (обобщённая матричная факторизация) \\
MLP   & Multi-Layer Perceptron (многослойный перцептрон) \\
RMSE  & Root Mean Squared Error (среднеквадратическая ошибка) \\
FedAvg & Federated Averaging (федеративное усреднение)
\end{tabularx}


% ВВЕДЕНИЕ
\structsection{ВВЕДЕНИЕ}
\addcontentsline{toc}{section}{ВВЕДЕНИЕ}

Рекомендательные системы широко применяются в потоковых сервисах, маркетплейсах и социальных сетях. При этом конфиденциальность пользовательских данных становится всё более важной: GDPR и Федеральный закон №~152-ФЗ <<О~персональных данных>> запрещают бесконтрольный сбор данных на один сервер. Федеративное обучение (ФО) решает эту проблему: модели обучаются локально, а на сервер передаются только обновления параметров, а не сами данные.

Однако на практике всё не так хорошо. Данные разных компаний распределены неравномерно (non-IID): одна обслуживает аудиторию любителей боевиков, другая --- драм, третья --- документалок. Стандартный FedAvg в таких условиях теряет достаточно качества. При этом у многих сервисов есть открытые или обезличенные данные (публичные рейтинги, демо-аккаунты), которыми можно безопасно делиться. Возникает вопрос: можно ли использовать такие публичные данные для стабилизации ФО, сохранив приватность остальных?

Именно это исследуется в данной НИР. Моделируется ситуация, в которой 20~компаний совместно обучают рекомендательную модель: 50\% пользователей публичные (их рейтинги доступны всем), 50\% приватные (данные не покидают компанию). Соотношение 50/50 выбрано потому, что это позволяет оценить эффект публичных данных при существенной доле приватных; в ВКР планируется исследовать и другие соотношения. 20~клиентов соответствуют cross-silo сценарию, типичному для индустрии рекомендаций.

Цель НИР: улучшить качество работы рекомендательной системы в условиях приватности данных.

Для этого поставлены следующие задачи:
\begin{enumerate}
  \item провести анализ подходов к федеративному обучению рекомендательных систем и методов борьбы с non-IID данными;
  \item формализовать задачу предсказания рейтингов с разделением пользователей на публичных и приватных;
  \item модифицировать архитектуру NCF (Neural Collaborative Filtering);
  \item реализовать программный прототип;
  \item провести экспериментальную оценку на открытом датасете.
\end{enumerate}

Математический аппарат: нейронная коллаборативная фильтрация (NCF), стохастическая оптимизация (Adam), федеративное усреднение (FedAvg). Реализация выполнена на Python с использованием PyTorch и Flower.

\newpage


\section{Аналитический обзор информационных источников}

\subsection{Рекомендательные системы: основные подходы}

Задача рекомендательной системы~\cite{ricci2015recommender}: по истории взаимодействий пользователя с объектами (фильмы, товары, музыка) предложить ему то, что ему понравится. Есть несколько способов это делать.

Контентная фильтрация смотрит на атрибуты самого объекта (жанр, режиссёр и т.д.) и сравнивает их с профилем пользователя. Работает, даже если у объекта ещё нет оценок, но при этом рекомендации быстро замыкаются на знакомых пользователю типах контента.

Коллаборативная фильтрация (КФ) устроена иначе: она ищет похожих пользователей и рекомендует то, что понравилось им. Если быть конкретнее, есть memory-based подход (считаем взвешенное среднее оценок соседей) и model-based (обучаем модель). Интерес представляет второй.

Ключевой model-based метод --- матричная факторизация. Koren и др.~\cite{koren2009matrix} предложили представлять каждого пользователя и каждый фильм вектором в~$\mathbb{R}^d$, а рейтинг предсказывать как скалярное произведение этих векторов плюс смещения: $\hat{r}_{ui} = \mathbf{p}_u^\top \mathbf{q}_i + b_u + b_i + \mu$. Подход выиграл Netflix Prize и до сих пор остаётся сильным baseline. Rendle~\cite{rendle2010factorization} обобщил эту идею до факторизационных машин (FM), которые работают с произвольными разреженными признаками, а не только с матрицей <<пользователь--объект>>.

На практике часто комбинируют контентный и коллаборативный подходы~\cite{ricci2015recommender}: например, один метод фильтрует кандидатов, другой ранжирует.


\subsection{Нейросетевые рекомендательные системы}

Матричная факторизация по определению линейна: она может уловить, что пользователю нравятся <<умные триллеры>>, но не сможет выразить более сложные зависимости~\cite{goodfellow2016deep}. Нейросети снимают это ограничение.

He и др.~\cite{he2017neural} предложили NCF (Neural Collaborative Filtering) --- архитектуру из двух ветвей. GMF-ветвь берёт эмбеддинги пользователя и фильма и перемножает их поэлементно, что по сути та же матричная факторизация. MLP-ветвь конкатенирует эмбеддинги и пропускает через полносвязные слои с нелинейными активациями. Выходы обеих ветвей объединяются и дают итоговое предсказание. На MovieLens и Pinterest NCF показала улучшение по сравнению с линейными baseline. Правда, Rendle и др.~\cite{rendle2020ncf} позднее показали, что хорошо настроенная матричная факторизация на тех же данных работает не хуже, так что часть выигрыша NCF была от более аккуратного подбора гиперпараметров. Это было учтено при подготовке экспериментов: параметры baseline подбирались тщательно.

Есть и другие подходы. AutoRec~\cite{sedhain2015autorec} использует автоэнкодер для восстановления неполного вектора рейтингов. NGCF~\cite{wang2019neural} и LightGCN~\cite{he2020lightgcn} строят граф взаимодействий и обогащают эмбеддинги через агрегацию по соседям; при этом LightGCN показал, что нелинейные преобразования не дают выигрыша --- хватает простого усреднения. Для последовательных рекомендаций (когда важен порядок: что смотрели вчера, что сегодня) SASRec~\cite{kang2018selfattentive} и BERT4Rec~\cite{sun2019bert4rec} применяют self-attention. Но для данной задачи (предсказание рейтинга без временной динамики) трансформеры избыточны.

NCF выбрана по трём причинам. Во-первых, она достаточно проста для отладки и экспериментов. Во-вторых, две ветви (линейная и нелинейная) удобны для интерпретации. В-третьих, параметры NCF естественно делятся на эмбеддинги (которые привязаны к конкретным пользователям) и веса сети (которые общие). Для ФО это ключевое свойство: эмбеддинги приватных пользователей можно оставить на клиенте, а остальное агрегировать.


\subsection{Метрики оценки рекомендательных систем}

Качество рекомендательных систем оценивается двумя группами метрик~\cite{ricci2015recommender}. \textbf{Регрессионные} (RMSE, MAE) показывают точность числового предсказания; RMSE сильнее штрафует за грубые промахи. \textbf{Ранкинговые} (HR@K, NDCG@K) оценивают, попадает ли нужный объект в top-$K$ списка и насколько высоко он стоит. На практике ранкинг часто важнее точного числа: пользователю всё равно, предсказала модель оценку 4.3 или 4.5, если фильм оказался в начале списка.

Для ранкинговых метрик используется протокол из~\cite{he2017neural}: к каждому тестовому объекту добавляется 99 случайных негативных, и модель ранжирует все 100.


\subsection{Федеративное обучение}

Идея ФО~\cite{mcmahan2017communication}: вместо того, чтобы собирать данные в одном месте, каждый участник обучает модель у себя и отправляет на сервер только обновлённые веса. Сервер усредняет их и рассылает обратно. Данные никуда не уходят.

Базовый алгоритм FedAvg работает так: на каждом раунде сервер отправляет текущие параметры $\theta^{(t)}$ случайной подвыборке клиентов, те обучаются $E$ эпох на своих данных, и сервер усредняет результат:
\begin{equation}
\theta^{(t+1)} = \frac{\sum_{k \in \mathcal{S}_t} |\mathcal{D}_k| \cdot \theta_k^{(t+1)}}{\sum_{k \in \mathcal{S}_t} |\mathcal{D}_k|}.
\end{equation}

Но есть нюанс --- если данные клиентов неоднородны (non-IID), локальные модели после нескольких эпох <<разъезжаются>> (client drift), и их среднее оказывается плохим для всех. По данным ряда экспериментов, потеря точности при сильном non-IID может достигать 50\%. Чем больше локальных эпох $E$, тем сильнее дрейф, но тем реже нужна связь с сервером. Приходится балансировать.

В рекомендательных системах non-IID --- норма. У разных компаний разная аудитория: один сервис обслуживает любителей драм, другой --- боевиков. Это сразу и label skew (разные распределения оценок), и feature skew (разные жанровые предпочтения), и quantity skew (разный объём данных).

\subsubsection{Что с этим делают}

FedProx~\cite{li2020federated} добавляет штраф за отклонение от глобальной модели:
\begin{equation}
\mathcal{L}_k^{\text{prox}}(\theta) = \mathcal{L}_k(\theta) + \frac{\mu}{2}\|\theta - \theta^{(t)}\|^2.
\end{equation}
Это не устраняет дрейф, но ограничивает его. SCAFFOLD~\cite{karimireddy2020scaffold} действует точнее: каждый клиент отслеживает, в какую сторону его градиент отличается от глобального, и корректирует обновление. В теории это решает проблему полностью, но на практике требует вдвое больше коммуникаций, что не всегда оправдано. Wang и др.~\cite{wang2020tackling} обнаружили ещё одну проблему: batch normalization усугубляет non-IID, потому что считает статистики по локальным данным. В используемой модели (NCF) batch normalization не применяется, поэтому данная проблема не актуальна.

Самый простой способ стабилизировать FedAvg --- дать клиентам немного общих данных. Даже 5--10\% выборки, доступной всем, заметно улучшают сходимость. Именно на этом основан предложенный подход: публичные пользователи доступны всем клиентам.

\subsubsection{Cross-silo и частичная агрегация}

ФО бывает двух видов. Cross-device --- миллионы смартфонов, каждый с крохотным объёмом данных. Cross-silo --- десятки организаций с большими и стабильными данными. Рассматриваемый сценарий --- cross-silo: 20~компаний, каждая со своими пользователями, стабильное подключение.

Для рекомендательных систем есть ещё одна удобная особенность: не все параметры имеет смысл агрегировать. Эмбеддинги приватных пользователей привязаны к конкретному клиенту, для других клиентов они бесполезны. Поэтому на сервер передаются только shared-параметры (веса сети, эмбеддинги объектов, эмбеддинги публичных пользователей), а приватные эмбеддинги остаются локально. Это одновременно экономит трафик и защищает данные.


\subsection{Приватность в федеративном обучении}

ФО не гарантирует приватность автоматически. Даже из обновлений весов можно извлечь информацию о данных клиента (gradient inversion attack). Для изображений такие атаки работают хорошо, но для рекомендательных систем угроза ниже: обновления эмбеддингов усредняют паттерны сотен пользователей.

Есть два основных механизма защиты. Дифференциальная приватность (DP) добавляет шум к параметрам, но ухудшает качество модели. Secure Aggregation шифрует обновления так, что сервер видит только их сумму, но требует квадратичных коммуникаций.

В данной работе не используется ни то, ни другое. Конфиденциальность обеспечена самой архитектурой: эмбеддинги приватных пользователей физически не покидают клиента. Shared-параметры (веса MLP, эмбеддинги фильмов, эмбеддинги публичных пользователей) не содержат информации о конкретных приватных пользователях. Даже если сервер скомпрометирован, приватные данные останутся на клиентах. Добавление DP планируется в ВКР.


\subsection{Федеративные рекомендательные системы}

Применение ФО к рекомендательным системам --- относительно молодое направление, и большинство работ опубликованы после 2019~года.

Первый шаг -- федеративная матричная факторизация. Ammad-ud-din и др.~\cite{ammad2019federated} предложили оставлять эмбеддинги пользователей локально, а эмбеддинги фильмов агрегировать на сервере. Идея естественная: пользователи принадлежат конкретным клиентам, а каталог фильмов общий. Chai и др.~\cite{chai2020secure} пошли дальше и добавили к этой схеме гомоморфное шифрование, но ценой значительных вычислительных затрат. Оба подхода ограничены линейной моделью и не улавливают нелинейные паттерны взаимодействий.

Нейросетевые варианты появились позже. FedRec~\cite{lin2020fedrec} добавляет к NCF дифференциальную приватность (зашумление градиентов), а FedFast~\cite{muhammad2020fedfast} ускоряет сходимость за счёт умного выбора клиентов на каждом раунде. Для персонализации Fallah и др.~\cite{fallah2020personalized} предложили Per-FedAvg на основе метаобучения: глобальная модель учится быть хорошей <<начальной точкой>>, от которой каждый клиент быстро адаптируется за 1--2 шага градиентного спуска. Есть и более простые подходы, например локальные <<головки>> поверх общего <<тела>> модели.

Общая проблема перечисленных работ в том, что они рассматривают полностью приватные данные. На практике же часто доступны открытые или обезличенные данные (публичные рейтинги, демо-аккаунты), которые могут использовать все участники. Именно этот \textit{гибридный сценарий} исследуется в данной работе: часть пользователей публичная, часть приватная. Публичные данные одновременно увеличивают обучающую выборку каждого клиента и стабилизируют агрегацию, уменьшая дрейф.

Сводное сравнение рассмотренных подходов представлено в таблице~\ref{tab:related_work}.

\begin{table}[H]
\caption{Сравнение подходов к федеративным рекомендательным системам}
\label{tab:related_work}
\centering
\small
\begin{tabularx}{\textwidth}{lccX}
\toprule
\textbf{Метод} & \textbf{Модель} & \textbf{Приватность} & \textbf{Особенности} \\
\midrule
FedMF~\cite{ammad2019federated}   & MF      & Архит.       & Первая адаптация MF для FL \\
FedMF+~\cite{chai2020secure}      & MF      & HE           & Гомоморфное шифрование градиентов \\
FedRec~\cite{lin2020fedrec}       & NCF     & DP           & Дифференциальная приватность \\
FedFast~\cite{muhammad2020fedfast}& NCF     & Архит.       & Активная выборка клиентов \\
Per-FedAvg~\cite{fallah2020personalized} & Любая & Архит.  & Мета-обучение для персонализации \\
\textbf{Предложенный}             & HybridNCF & Архит.     & Публичные + приватные эмбеддинги \\
\bottomrule
\end{tabularx}
\end{table}


\subsection{Набор данных MovieLens}

MovieLens --- семейство датасетов от GroupLens Research (Университет Миннесоты)~\cite{harper2015movielens}: MovieLens-100K, 1M, 10M и 20M.

Используется MovieLens-1M: 1~000~209 оценок 3~706 фильмов от 6~040 пользователей, шкала от~1 до~5. Набор выбран по двум основным причинам: жанровая разметка (18 жанров) позволяет моделировать non-IID распределение, а объём достаточен для значимых результатов при запуске на CPU. Разреженность матрицы ($\sim$4.5\%) типична для реальных сервисов.

Статистика набора представлена в таблице~\ref{tab:movielens_stats}.

\begin{table}[H]
\caption{Статистика набора данных MovieLens-1M}
\label{tab:movielens_stats}
\centering
\begin{tabular}{lc}
\toprule
\textbf{Характеристика} & \textbf{Значение} \\
\midrule
Количество пользователей & 6~040 \\
Количество фильмов       & 3~706 \\
Количество оценок         & 1~000~209 \\
Шкала оценок              & 1--5 (целые) \\
Средняя оценка            & $\sim$3.58 \\
Медиана оценок на пользователя & $\sim$96 \\
Плотность матрицы         & $\sim$4.5\% \\
Количество жанров         & 18 \\
\bottomrule
\end{tabular}
\end{table}

Оценки распределены неравномерно: тройки и четвёрки составляют больше 55\%. Это ожидаемо --- люди чаще оценивают то, что им понравилось. По числу оценок на пользователя разброс тоже большой: минимум 20 (условие фильтрации в самом наборе), максимум больше 2~000.

Важна жанровая разметка: она позволяет создать реалистичное non-IID распределение. Каждому из 20 клиентов назначается пара жанров, и пользователи с соответствующими вкусами чаще попадают к этому клиенту. Получается ситуация, близкая к реальной: разные компании обслуживают разную аудиторию. При этом набор достаточно большой для осмысленных результатов и достаточно маленький, чтобы все эксперименты запускались на CPU.


\subsection{Инструменты и фреймворки}

Реализация основана на PyTorch~\cite{paszke2019pytorch} (определение модели, автодифференцирование, оптимизация) и Flower~\cite{beutel2020flower} (симуляция ФО, собственная стратегия агрегации). Flower выбран потому, что позволяет определять свои стратегии агрегации, что необходимо для Hybrid Partial FedAvg, где агрегируется только часть параметров. Вспомогательные библиотеки: NumPy, Pandas (предобработка MovieLens), Matplotlib (визуализация).


\subsection{Выводы по обзору}

Подведём итог. NCF хорошо подходит для ФО, потому что её параметры делятся на эмбеддинги (привязаны к конкретным пользователям) и веса сети (общие). Существующие федеративные рекомендательные системы (FedMF, FedRec, FedFast) работают с полностью приватными данными, а гибридный сценарий с публичными и приватными пользователями почти не исследован. При этом именно общие данные --- самый простой и эффективный способ бороться с non-IID.

Отсюда и идея работы: взять NCF, разделить параметры на shared и private, добавить публичных пользователей для стабилизации и посмотреть, можно ли так получить качество не хуже обучения на объединённых данных.

\newpage


\section{Содержательная постановка задачи}

\textbf{Предметная область.}
Рассматривается задача построения рекомендательной системы в распределённой среде, состоящей из нескольких организаций (клиентов), каждая из которых обладает собственным набором данных об оценках фильмов пользователями. Необходимо обучить единую модель предсказания рейтингов, не передавая сырые данные ни одному из участников и не отправляя их на центральный сервер.

\textbf{Объект исследования:} рекомендательная система, основанная на нейронной коллаборативной фильтрации.

\textbf{Предмет исследования:} метод гибридного федеративного обучения с разделением параметров модели на агрегируемые (публичные) и локальные (приватные).

\textbf{Исходные данные.}
Набор данных MovieLens-1M: 6~040 пользователей, 3~706 фильмов, 1~000~209 оценок по пятибалльной шкале. Пользователи разделены на два подмножества:
\begin{itemize}
  \item \textit{публичные пользователи} (50\%, 3~020 человек), чьи данные доступны всем клиентам (моделируют обезличенные или открытые данные);
  \item \textit{приватные пользователи} (50\%, 3~020 человек), чьи данные принадлежат только одному клиенту и никогда не покидают его.
\end{itemize}

Приватные пользователи распределяются по 20 клиентам неравномерно (non-IID) на основе жанровых предпочтений.

\textbf{Основная цель.}
Разработать метод федеративного обучения рекомендательной модели, обеспечивающий:
\begin{enumerate}
  \item качество предсказания рейтингов (RMSE), сопоставимое с обучением на объединённых данных;
  \item сохранение конфиденциальности: сырые данные приватных пользователей не покидают клиента;
  \item устойчивость к гетерогенности (non-IID) данных клиентов.
\end{enumerate}

\textbf{Ожидаемый результат.}
Программный прототип системы федеративного обучения, экспериментальная оценка предложенного метода и сравнение с обучением на объединённых данных.

\textbf{Математический аппарат:}
нейронная коллаборативная фильтрация (NCF), методы стохастической оптимизации (Adam), алгоритм федеративного усреднения (FedAvg).

\textbf{Программные средства:}
Python~3.10, PyTorch~2.x, Flower~1.x, NumPy, Pandas, Matplotlib.

\newpage


\section{Математическая постановка задачи}

\subsection{Формализация задачи предсказания рейтингов}

По сути решается задача регрессии: есть пользователи $\mathcal{U}$, фильмы $\mathcal{I}$ и набор известных оценок $\mathcal{R} = \{(u, i, r_{ui})\}$ по шкале от 1 до 5. Нужно научиться предсказывать оценку для пар, которых нет в обучающей выборке.

Формально ищем функцию $\hat{r} : \mathcal{U} \times \mathcal{I} \to [1, 5]$, минимизирующую MSE:
\begin{equation}
\min_{\theta} \; \mathcal{L}(\theta) = \frac{1}{|\mathcal{R}|} \sum_{(u, i, r_{ui}) \in \mathcal{R}} \bigl( \hat{r}(u, i;\, \theta) - r_{ui} \bigr)^2,
\label{eq:mse}
\end{equation}
здесь $\theta$ --- вектор параметров модели.


\subsection{Федеративная постановка}

Теперь добавим к этой задаче федеративную структуру. Есть $K = 20$ клиентов. Пользователи делятся на два непересекающихся множества:
\begin{equation}
\mathcal{U} = \mathcal{U}^{\text{pub}} \cup \mathcal{U}^{\text{priv}}, \quad \mathcal{U}^{\text{pub}} \cap \mathcal{U}^{\text{priv}} = \varnothing.
\end{equation}

Данные публичных пользователей $\mathcal{R}^{\text{pub}} = \{(u, i, r) \mid u \in \mathcal{U}^{\text{pub}}\}$ доступны всем клиентам. Данные приватных пользователей распределены по клиентам:
\begin{equation}
\mathcal{U}^{\text{priv}} = \bigsqcup_{k=1}^{K} \mathcal{U}_k^{\text{priv}}, \quad \mathcal{U}_j^{\text{priv}} \cap \mathcal{U}_l^{\text{priv}} = \varnothing \;\; \forall j \neq l.
\end{equation}

Ключевой момент: параметры модели на каждом клиенте состоят из двух частей:
\begin{equation}
\theta = \theta^{\text{shared}} \cup \theta^{\text{private}}_k \;\;\text{для клиента } k.
\end{equation}
Shared-параметры ($\theta^{\text{shared}}$) --- это эмбеддинги публичных пользователей, эмбеддинги фильмов, веса MLP и выходного слоя. Они одинаковы на всех клиентах и агрегируются на сервере. Private-параметры ($\theta^{\text{private}}_k$) --- эмбеддинги приватных пользователей клиента~$k$. Они никуда не отправляются.


\subsection{Алгоритм Hybrid Partial FedAvg}

На каждом раунде $t = 1, \ldots, T$ выполняются следующие шаги:

\begin{enumerate}
  \item \textbf{Рассылка:} сервер отправляет $\theta^{\text{shared},(t-1)}$ подмножеству клиентов $\mathcal{S}_t \subseteq \{1, \ldots, K\}$, $|\mathcal{S}_t| = \lfloor K \cdot C \rfloor$, при $C = 0.5$ (доля участвующих клиентов).

  \item \textbf{Локальное обучение:} каждый клиент $k \in \mathcal{S}_t$ выполняет $E = 2$ эпохи оптимизации на своих данных $\mathcal{D}_k = \mathcal{R}^{\text{pub}} \cup \mathcal{R}_k^{\text{priv}}$:
  \begin{equation}
    \theta_k^{(t)} \leftarrow \theta_k^{(t-1)} - \eta \nabla_\theta \mathcal{L}_k(\theta_k^{(t-1)}),
  \end{equation}
  здесь $\eta = 10^{-3}$ --- скорость обучения, $\mathcal{L}_k$ --- функция потерь~(\ref{eq:mse}) на данных клиента~$k$.

  \item \textbf{Агрегация:} сервер усредняет только разделяемые параметры:
  \begin{equation}
    \theta^{\text{shared},(t)} = \frac{\sum_{k \in \mathcal{S}_t} n_k \cdot \theta_k^{\text{shared},(t)}}{\sum_{k \in \mathcal{S}_t} n_k},
    \label{eq:fedavg}
  \end{equation}
  здесь $n_k = |\mathcal{D}_k|$ --- размер обучающей выборки клиента~$k$.

  \item \textbf{Кэширование:} приватные эмбеддинги $\theta^{\text{private}}_k$ сохраняются локально на диске клиента для использования в следующем раунде.
\end{enumerate}


\subsection{Non-IID распределение данных}

Чтобы эксперимент был реалистичным, приватных пользователей нужно распределить неравномерно. Каждому клиенту $k$ назначается пара жанров $(g_k^{\text{main}}, g_k^{\text{sec}})$. Для каждого приватного пользователя $u \in \mathcal{U}^{\text{priv}}$ вычисляется предпочтительный жанр $g_u$ на основе его оценок. Вероятность назначения пользователя $u$ клиенту $k$ определяется как:
\begin{equation}
P(u \to k) \propto \begin{cases}
\alpha, & \text{если } g_u \in \{g_k^{\text{main}}, g_k^{\text{sec}}\}, \\
\beta_k, & \text{иначе},
\end{cases}
\end{equation}
здесь $\alpha \sim 0.6 + U(-0.1, 0.1)$ --- концентрация жанровых предпочтений с небольшим шумом, а $\beta_k$ определяется из распределения Дирихле с параметром $\alpha_{\text{Dir}} = \max(0.3,\; 1 - 0.5 \cdot q)$ при $q = 0.5$ (параметр количественной неравномерности).


\subsection{Метрики качества}

Для оценки качества используются следующие метрики.

\textit{Регрессионные метрики} (на всех тестовых примерах):
\begin{equation}
\text{RMSE} = \sqrt{\frac{1}{|\mathcal{R}_{\text{test}}|}\sum_{(u,i,r)\in\mathcal{R}_{\text{test}}}(\hat{r}_{ui} - r_{ui})^2},
\end{equation}
\begin{equation}
\text{MAE} = \frac{1}{|\mathcal{R}_{\text{test}}|}\sum_{(u,i,r)\in\mathcal{R}_{\text{test}}}|\hat{r}_{ui} - r_{ui}|.
\end{equation}

\textit{Ранкинговые метрики} (протокол с негативной выборкой, 99 негативных примеров на каждый позитивный):
\begin{equation}
\text{HR@}K = \frac{1}{|\mathcal{R}_{\text{test}}|}\sum_{(u,i)\in\mathcal{R}_{\text{test}}} \mathbb{1}\bigl[i \in \text{Top-}K(u)\bigr],
\end{equation}
\begin{equation}
\text{NDCG@}K = \frac{1}{|\mathcal{R}_{\text{test}}|}\sum_{(u,i)\in\mathcal{R}_{\text{test}}} \frac{\mathbb{1}\bigl[i \in \text{Top-}K(u)\bigr]}{\log_2(\text{pos}(i, u) + 2)},
\end{equation}
здесь $\text{pos}(i, u)$ --- позиция истинного объекта $i$ в ранжированном списке кандидатов для пользователя~$u$.

\newpage


\section{Разработка математического обеспечения НИР}

\subsection{Обоснование выбора математического аппарата}

Задача предсказания рейтинга сводится к регрессии в пространстве латентных признаков~\cite{vorontsov2020ml}. Математически она включает:
\begin{itemize}
  \item линейную алгебру: пользователи и объекты представлены векторами в~$\mathbb{R}^d$;
  \item нелинейную аппроксимацию (MLP для моделирования нелинейных взаимодействий);
  \item стохастическую оптимизацию (Adam~\cite{kingma2015adam});
  \item распределённые вычисления (ФО для оптимизации над данными нескольких клиентов).
\end{itemize}

NCF выбрана потому, что GMF-ветвь ловит линейные паттерны, MLP --- нелинейные, а параметры модели естественно делятся на агрегируемые (эмбеддинги объектов, веса MLP) и локальные (эмбеддинги приватных пользователей).


\subsection{Модель NCF}

Функция предсказания рейтинга в архитектуре Neural Collaborative Filtering~\cite{he2017neural} определяется как:
\begin{equation}
\hat{r}(u, i;\, \theta) = \sigma\!\left( \mathbf{h}^\top \begin{bmatrix} \mathbf{p}_u^{G} \odot \mathbf{q}_i^{G} \\ \phi(\mathbf{p}_u^{M} \oplus \mathbf{q}_i^{M}) \end{bmatrix} \right) \cdot 4 + 1,
\label{eq:ncf}
\end{equation}
где:
\begin{itemize}
  \item $\mathbf{p}_u^{G}, \mathbf{p}_u^{M} \in \mathbb{R}^d$ --- эмбеддинги пользователя $u$ для GMF- и MLP-ветвей;
  \item $\mathbf{q}_i^{G}, \mathbf{q}_i^{M} \in \mathbb{R}^d$ --- эмбеддинги объекта $i$;
  \item $\odot$ --- поэлементное произведение (ветвь GMF);
  \item $\oplus$ --- конкатенация (ветвь MLP);
  \item $\phi(\cdot)$ --- многослойный перцептрон с архитектурой $[128, 64, 32]$ и активацией ReLU;
  \item $\sigma(\cdot)$ --- сигмоидная функция;
  \item $\mathbf{h} \in \mathbb{R}^{d+32}$ --- вектор весов выходного слоя.
\end{itemize}

Используется $d = 64$. Сигмоида на выходе даёт значение в $[0, 1]$, которое масштабируется в $[1, 5]$ умножением на 4 и прибавлением 1. Можно было бы использовать линейный выход с клипированием, но сигмоида даёт более гладкие градиенты вблизи границ шкалы.


\subsection{Архитектура модели HybridNCF}

HybridNCF отличается от обычной NCF одной вещью: вместо одной таблицы эмбеддингов пользователей у неё две --- для публичных и для приватных. При прямом проходе модель смотрит на флаг \texttt{is\_public} и выбирает нужную таблицу. Общая структура модели представлена на рисунке~\ref{fig:architecture}.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
  block/.style={draw, rounded corners, minimum width=2.5cm, minimum height=0.8cm, align=center, font=\small},
  emb/.style={draw, fill=blue!10, rounded corners, minimum width=2cm, minimum height=0.7cm, align=center, font=\footnotesize},
  privemb/.style={draw, fill=red!10, rounded corners, minimum width=2cm, minimum height=0.7cm, align=center, font=\footnotesize},
  arr/.style={-{Stealth[length=3mm]}, thick},
  >=Stealth
]

% Input
\node[block, fill=gray!10] (input) at (0,0) {Вход: $(u, i, \text{is\_pub})$};

% Public/Private embeddings
\node[emb] (pub_gmf) at (-4, 2) {Публ. user\\emb (GMF)};
\node[emb] (pub_mlp) at (-1.5, 2) {Публ. user\\emb (MLP)};
\node[privemb] (priv_gmf) at (1.5, 2) {Прив. user\\emb (GMF)};
\node[privemb] (priv_mlp) at (4, 2) {Прив. user\\emb (MLP)};

\node[emb] (item_gmf) at (-4, 3.5) {Item emb\\(GMF)};
\node[emb] (item_mlp) at (4, 3.5) {Item emb\\(MLP)};

% GMF and MLP
\node[block, fill=green!10] (gmf) at (-4, 5) {GMF:\\$\mathbf{p}_u \odot \mathbf{q}_i$};
\node[block, fill=yellow!10] (mlp) at (4, 5) {MLP:\\$[128, 64, 32]$};

% Concat
\node[block, fill=orange!10] (concat) at (0, 6.5) {Конкатенация + Linear};

% Output
\node[block, fill=gray!10] (output) at (0, 8) {$\hat{r} = \sigma(\cdot) \cdot 4 + 1$};

% Arrows
\draw[arr] (input) -- (pub_gmf);
\draw[arr] (input) -- (pub_mlp);
\draw[arr] (input) -- (priv_gmf);
\draw[arr] (input) -- (priv_mlp);

\draw[arr] (pub_gmf) -- (gmf);
\draw[arr] (priv_gmf) -- (gmf);
\draw[arr] (item_gmf) -- (gmf);

\draw[arr] (pub_mlp) -- (mlp);
\draw[arr] (priv_mlp) -- (mlp);
\draw[arr] (item_mlp) -- (mlp);

\draw[arr] (gmf) -- (concat);
\draw[arr] (mlp) -- (concat);
\draw[arr] (concat) -- (output);

% Legend
\node[emb, minimum width=1cm, minimum height=0.5cm] at (-4, -1) {};
\node[font=\footnotesize, right] at (-3.3, -1) {Агрегируемые};
\node[privemb, minimum width=1cm, minimum height=0.5cm] at (1, -1) {};
\node[font=\footnotesize, right] at (1.7, -1) {Локальные};

\end{tikzpicture}
\caption{Архитектура модели HybridNCF с разделением эмбеддингов}
\label{fig:architecture}
\end{figure}

Общее число агрегируемых параметров модели составляет примерно 888~тысяч:
\begin{itemize}
  \item эмбеддинги публичных пользователей: $3\,020 \times 64 \times 2 = 386\,560$;
  \item эмбеддинги объектов: $3\,706 \times 64 \times 2 = 474\,368$;
  \item веса MLP и выходного слоя: $\sim$27\,000.
\end{itemize}
Дополнительно каждый клиент хранит локальные эмбеддинги приватных пользователей: от $\sim$6\,000 до $\sim$20\,000 параметров в зависимости от количества приватных пользователей на клиенте.


\subsection{Алгоритм оптимизации}

Для оптимизации параметров используется алгоритм Adam~\cite{kingma2015adam} с параметрами: скорость обучения $\eta = 10^{-3}$, $\beta_1 = 0.9$, $\beta_2 = 0.999$, $\varepsilon = 10^{-8}$, $L_2$-регуляризация $\lambda = 10^{-3}$. Размер мини-батча составляет 256.

Инициализация параметров: эмбеддинги из $\mathcal{N}(0, 0.01^2)$; веса линейных слоёв по Xavier uniform; смещения нулевые.


\subsection{Протокол федеративного обучения}

Общая схема взаимодействия сервера и клиентов показана на рисунке~\ref{fig:fl_protocol}.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
  arr/.style={-{Stealth[length=3mm]}, thick},
  server/.style={draw, thick, fill=blue!10, rounded corners,
    minimum width=11cm, minimum height=1.2cm, align=center, font=\small},
  client/.style={draw, thick, fill=gray!6, rounded corners,
    minimum width=3cm, minimum height=3.2cm},
  pub/.style={draw, fill=green!10, rounded corners,
    minimum width=2.6cm, minimum height=0.6cm, align=center, font=\scriptsize},
  priv/.style={draw, dashed, fill=red!8, rounded corners,
    minimum width=2.6cm, minimum height=0.6cm, align=center, font=\scriptsize},
  pubsrc/.style={draw, thick, fill=green!15, rounded corners,
    minimum width=11cm, minimum height=0.9cm, align=center, font=\small}
]

% Сервер
\node[server] (server) at (0, 0) {\textbf{Сервер}\\агрегация shared-параметров};

% Стрелки сервер <-> клиенты
\draw[arr, blue] (-3.8, -0.7) -- (-3.8, -2.0)
  node[midway, left, font=\scriptsize] {shared $\downarrow$};
\draw[arr, red!70!black] (-3.0, -2.0) -- (-3.0, -0.7)
  node[midway, right, font=\scriptsize] {$\uparrow$ обнов.};

\draw[arr, blue] (-0.4, -0.7) -- (-0.4, -2.0);
\draw[arr, red!70!black] (0.4, -2.0) -- (0.4, -0.7);

\draw[arr, blue] (3.0, -0.7) -- (3.0, -2.0);
\draw[arr, red!70!black] (3.8, -2.0) -- (3.8, -0.7);

% Клиенты
\node[client] (c1) at (-3.4, -3.9) {};
\node[client] (c2) at (0, -3.9) {};
\node[client] (c3) at (3.4, -3.9) {};

\node[font=\scriptsize\bfseries] at (-3.4, -2.5) {Клиент 1};
\node[font=\scriptsize\bfseries] at (0, -2.5) {Клиент $i$};
\node[font=\scriptsize\bfseries] at (3.4, -2.5) {Клиент $n$};

% Многоточие
\node[font=\large] at (1.7, -3.9) {\ldots};

% Содержимое клиентов
\node[priv] at (-3.4, -3.4) {приватные};
\node[pub] at (-3.4, -4.4) {публичные};

\node[priv] at (0, -3.4) {приватные};
\node[pub] at (0, -4.4) {публичные};

\node[priv] at (3.4, -3.4) {приватные};
\node[pub] at (3.4, -4.4) {публичные};

% Публичные данные внизу
\node[pubsrc] (pubdata) at (0, -6.5)
  {Публичные данные --- общий набор, доступный всем клиентам};

% Стрелки от публичных данных к клиентам
\draw[arr, green!50!black, dashed] (pubdata.north) +(-3.4, 0) -- +(-3.4, 0.8);
\draw[arr, green!50!black, dashed] (pubdata.north) +(0, 0) -- +(0, 0.8);
\draw[arr, green!50!black, dashed] (pubdata.north) +(3.4, 0) -- +(3.4, 0.8);

\end{tikzpicture}
\caption{Схема Hybrid Partial FedAvg: публичные данные доступны всем клиентам, приватные остаются локально, агрегируются только shared-параметры}
\label{fig:fl_protocol}
\end{figure}

\newpage


\section{Использование методов и средств информационных технологий}

\subsection{Выбор программных средств}

Для реализации прототипа системы федеративного обучения были выбраны следующие инструменты:

\begin{itemize}
  \item \textbf{Python 3.10} --- основной язык разработки;
  \item \textbf{PyTorch 2.x} --- фреймворк глубокого обучения (автодифференцирование, динамический граф вычислений);
  \item \textbf{Flower 1.x} --- фреймворк ФО с абстракциями клиента и сервера, поддержкой пользовательских стратегий агрегации;
  \item \textbf{NumPy, Pandas} --- работа с числовыми и табличными данными;
  \item \textbf{Matplotlib, Seaborn} --- визуализация результатов;
  \item \textbf{Jupyter Notebook} --- интерактивный анализ экспериментов.
\end{itemize}

Целевая платформа: ОС Linux, процессор x86\_64. Вычисления проводились на CPU (адаптация под GPU предусмотрена в коде, но для объёмов MovieLens-1M не является необходимой).


\subsection{Структура программного проекта}

Проект организован по модульному принципу:

\begin{verbatim}
fed_recsys/
  configs/
    config.yaml           -- гиперпараметры
  src/
    data/
      download.py          -- загрузка MovieLens
      splitter.py          -- non-IID распределение
    models/
      ncf.py               -- NCF и HybridNCF
    federated/
      client.py            -- FL-клиент (Flower)
      server.py            -- FL-сервер и оценка
    utils/
      metrics.py           -- RMSE, MAE, HR@K, NDCG@K
  scripts/
    prepare_data.py        -- подготовка данных
    train_centralized.py   -- обучение на объединённых данных
    run_simulation.py      -- запуск FL-симуляции
  notebooks/
    experiments.ipynb      -- анализ результатов
\end{verbatim}


\subsection{Конфигурация эксперимента}

Все гиперпараметры вынесены в единый файл \texttt{config.yaml}. Основные параметры приведены в таблице~\ref{tab:config}.

\begin{table}[H]
\caption{Основные гиперпараметры эксперимента}
\label{tab:config}
\centering
\begin{tabularx}{\textwidth}{llX}
\toprule
\textbf{Параметр} & \textbf{Значение} & \textbf{Описание} \\
\midrule
\texttt{num\_clients}     & 20    & Количество клиентов \\
\texttt{public\_user\_ratio} & 0.5 & Доля публичных пользователей \\
\texttt{embedding\_dim}   & 64    & Размерность эмбеддингов \\
\texttt{mlp\_layers}      & [128, 64, 32] & Архитектура MLP \\
\texttt{dropout}          & 0.2   & Вероятность dropout \\
\texttt{batch\_size}      & 256   & Размер мини-батча \\
\texttt{learning\_rate}   & 0.001 & Скорость обучения \\
\texttt{weight\_decay}    & 0.001 & $L_2$-регуляризация \\
\texttt{local\_epochs}    & 2     & Локальных эпох на раунд \\
\texttt{num\_rounds}      & 20    & Количество раундов FL \\
\texttt{fraction\_fit}    & 0.5   & Доля клиентов на раунд \\
\texttt{centralized\_epochs} & 50 & Эпох общего обучения \\
\texttt{genre\_concentration} & 0.6 & Концентрация жанров (non-IID) \\
\texttt{quantity\_imbalance}  & 0.5 & Неравномерность объёмов данных \\
\bottomrule
\end{tabularx}
\end{table}

Большинство гиперпараметров выбраны как стандартные значения из оригинальных работ.
Размерность эмбеддингов $d=64$ и архитектура MLP $[128, 64, 32]$ соответствуют конфигурации NCF из~\cite{he2017neural}.
Оптимизатор Adam со скоростью обучения $\eta=10^{-3}$ и параметрами $\beta_1=0{,}9$, $\beta_2=0{,}999$ использует значения по умолчанию~\cite{kingma2015adam}.
Dropout $0{,}2$ и $L_2$-регуляризация $10^{-3}$ --- типичные значения для предотвращения переобучения на разреженных данных рекомендательных систем.
Федеративные параметры --- 20~раундов, 2~локальные эпохи, доля клиентов 50\,\% --- следуют рекомендациям McMahan и др.~\cite{mcmahan2017communication}.
Размер батча 256 выбран как компромисс между скоростью сходимости и стабильностью градиентов для датасета данного масштаба (${\sim}1$~млн оценок).

\subsection{Концепт программной системы}

На рисунке~\ref{fig:pipeline} представлена общая схема работы системы.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
  stage/.style={draw, rounded corners, fill=blue!8, minimum width=4cm, minimum height=1cm, align=center, font=\small},
  arr/.style={-{Stealth[length=3mm]}, thick},
]

\node[stage] (s1) at (0, 0)   {1. Подготовка данных\\(\texttt{prepare\_data.py})};
\node[stage] (s2) at (0, -2)  {2. Общее обучение\\(\texttt{train\_centralized.py})};
\node[stage] (s3) at (0, -4)  {3. Федеративная симуляция\\(\texttt{run\_simulation.py})};
\node[stage] (s4) at (0, -6)  {4. Анализ результатов\\(\texttt{experiments.ipynb})};

\draw[arr] (s1) -- (s2);
\draw[arr] (s2) -- (s3);
\draw[arr] (s3) -- (s4);

\end{tikzpicture}
\caption{Конвейер выполнения экспериментов}
\label{fig:pipeline}
\end{figure}

\newpage


\section{Проведённые исследования и анализ полученных результатов}

\subsection{Распределение данных по клиентам}

После non-IID разбиения объём данных по клиентам получился сильно разным: от~50 до~326 приватных пользователей (разброс в $\sim$6.5 раз). Публичные данные (3~020 пользователей) одинаковы у всех.

На рисунке~\ref{fig:data_dist} видно, как распределились данные: слева --- число приватных пользователей на клиента, справа --- какие жанры достались каждому клиенту.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{data_distribution.png}
\caption{Распределение данных по клиентам: количество приватных пользователей (слева) и жанровое распределение (справа)}
\label{fig:data_dist}
\end{figure}

На heatmap видно, что каждый клиент специализируется на 1--2 жанрах --- non-IID распределение достигнуто.


\subsection{Результаты общего обучения (baseline)}

Сначала была обучена обычная NCF на всех данных без разбиения по клиентам (50 эпох). Лучший RMSE на валидации --- на 25-й эпохе, дальше модель начала переобучаться. Итоговые метрики на тесте: RMSE = 0.8884, MAE = 0.7026, HR@10 = 0.3431, NDCG@10 = 0.1832. Это близко к тому, что получают в литературе для NCF на MovieLens-1M~\cite{he2017neural}, так что baseline адекватный.


\subsection{Результаты федеративного обучения}

Федеративное обучение: 20 раундов, на каждом раунде 10 из 20 клиентов, по 2 локальные эпохи. Кривые обучения на рисунке~\ref{fig:curves}.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{comparison_curves.png}
\caption{Кривые обучения: Train Loss (слева) и RMSE на валидации (справа) для общего и федеративного подходов}
\label{fig:curves}
\end{figure}

Видно, что федеративная модель сходится плавнее: нет резкого перелома на кривой loss, нет переобучения. Это логично --- усреднение обновлений от 10 разных клиентов само по себе действует как регуляризация.


\subsection{Сравнительный анализ метрик}

Итоговые метрики обоих подходов приведены в таблице~\ref{tab:comparison}.

\begin{table}[H]
\caption{Сравнение метрик качества на тестовой выборке}
\label{tab:comparison}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Метрика} & \textbf{Общее} & \textbf{Федеративное} & \textbf{Разница} \\
\midrule
RMSE    & 0.8884 & 0.8652 & $-2.6\%$ \\
MAE     & 0.7026 & 0.6831 & $-2.8\%$ \\
HR@10   & 0.3431 & 0.2898 & $-15.5\%$ \\
NDCG@10 & 0.1832 & 0.1500 & $-18.1\%$ \\
\bottomrule
\end{tabular}
\end{table}

Визуальное сравнение метрик представлено на рисунке~\ref{fig:metrics}.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{comparison_metrics.png}
\caption{Столбчатая диаграмма сравнения метрик: общее vs федеративное обучение}
\label{fig:metrics}
\end{figure}


\subsection{Анализ результатов}

Главный неожиданный результат: \textbf{по RMSE и MAE федеративная модель лучше, чем общее обучение}. Почему так?

Публичные пользователи (3~020 из 6~040) есть на всех 20 клиентах. Каждый клиент обучает их эмбеддинги на своих данных (со своим жанровым уклоном), а сервер усредняет обновления от 10 клиентов за раунд. Получается эффект ансамбля: итоговые эмбеддинги оказываются устойчивее к шуму, чем при обучении одной моделью. Аналогично с эмбеддингами фильмов: каждый клиент видит фильм «через свою призму», и усреднение даёт более обобщённое представление.

При этом \textbf{ранкинговые метрики (HR@10, NDCG@10) у ФО хуже на 15--18\%}. Причина понятна: ранкинг считается на приватных пользователях, чьи эмбеддинги не агрегируются. Каждый клиент обучает $\sim$150 приватных пользователей только на локальных данных, поэтому их эмбеддинги менее точные, чем при обучении на всех данных сразу. Это ожидаемая плата за приватность.


\subsection{Стабильность по клиентам}

RMSE по отдельным клиентам: от 0.860 до 0.869, коэффициент вариации $\sim$0.4\%. Несмотря на то что у одного клиента 50 приватных пользователей, а у другого 326, качество модели почти одинаковое. Публичные данные эффективно стабилизируют обучение и компенсируют гетерогенность.

Для практики это означает, что все участники федерации получают модель одинакового качества независимо от объёма своих данных.

На рисунке~\ref{fig:convergence} представлена динамика сходимости федеративного обучения (слева) и значения RMSE для каждого из 20 клиентов (справа).

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{fl_convergence.png}
\caption{Сходимость федеративного обучения: валидационный RMSE по раундам (слева) и RMSE на тесте для каждого клиента (справа)}
\label{fig:convergence}
\end{figure}

% ВЫВОДЫ
\structsection{ВЫВОДЫ}
\addcontentsline{toc}{section}{ВЫВОДЫ}

Основные результаты НИР:

Проведён обзор подходов к рекомендательным системам, ФО и их пересечению. Сформулирована задача предсказания рейтингов в федеративной постановке с публичными и приватными пользователями (задача регрессии в латентном пространстве).

Разработана модель HybridNCF (расширение NCF с раздельными эмбеддингами для публичных и приватных пользователей). Предложен алгоритм Hybrid Partial FedAvg: агрегируются только shared-параметры, приватные эмбеддинги остаются на клиентах.

Реализован прототип на Python (PyTorch + Flower): подготовка данных с non-IID распределением, общее обучение, федеративная симуляция и анализ.

Эксперименты на MovieLens-1M (6~040 пользователей, 3~706 фильмов, $\sim$1~млн рейтингов, 20 клиентов):
\begin{itemize}
  \item RMSE = 0.865, MAE = 0.683: федеративная модель на 2.6--2.8\% лучше общего обучения (RMSE = 0.888, MAE = 0.703) за счёт ансамблирования публичных эмбеддингов;
  \item HR@10 = 0.290, NDCG@10 = 0.150: хуже общего обучения на 15--18\%, т.к. приватные эмбеддинги обучаются только на локальных данных (ожидаемая плата за приватность);
  \item коэффициент вариации RMSE по клиентам $\sim$0.4\%, т.е. все клиенты получают модель одинакового качества.
\end{itemize}

Цели НИР достигнуты: федеративная модель не уступает (и даже превосходит по RMSE) обучению на объединённых данных, при этом сырые данные не покидают клиентов.

В ходе НИР были освоены: архитектура NCF и её реализация на PyTorch, фреймворк Flower для симуляции ФО, методы non-IID разбиения данных, а также практика проведения воспроизводимых экспериментов с контролем гиперпараметров.

В целом НИР выполнена успешно: все поставленные задачи решены, основная цель достигнута. Полученные результаты составляют основу для дальнейшей подготовки ВКР.

Планы на ВКР:
\begin{itemize}
  \item исследовать влияние доли публичных пользователей;
  \item добавить FedProx для борьбы с client drift;
  \item реализовать дифференциальную приватность;
  \item эксперименты на MovieLens-10M и Amazon Reviews.
\end{itemize}

\newpage


% СПИСОК ИСПОЛЬЗОВАННЫХ ИСТОЧНИКОВ
\structsection{СПИСОК ИСПОЛЬЗОВАННЫХ ИСТОЧНИКОВ}
\addcontentsline{toc}{section}{СПИСОК ИСПОЛЬЗОВАННЫХ ИСТОЧНИКОВ}

\renewcommand{\refname}{}
\begin{thebibliography}{99}
\vspace{-2em}

\bibitem{ricci2015recommender}
Ricci~F., Rokach~L., Shapira~B.
Recommender Systems Handbook.
--- 2nd~ed. --- Springer, 2015. --- URL: \url{https://arxiv.org/abs/1707.07435}.

\bibitem{koren2009matrix}
Koren~Y., Bell~R., Volinsky~C.
Matrix Factorization Techniques for Recommender Systems~//
Computer. --- 2009. --- Vol.~42, No.~8. --- P.~30--37. --- URL: \url{https://doi.org/10.1109/MC.2009.263}.

\bibitem{rendle2010factorization}
Rendle~S.
Factorization Machines~//
Proc. IEEE Int. Conf. on Data Mining (ICDM). --- 2010. --- P.~995--1000. --- URL: \url{https://doi.org/10.1109/ICDM.2010.127}.

\bibitem{he2017neural}
He~X., Liao~L., Zhang~H., Nie~L., Hu~X., Chua~T.-S.
Neural Collaborative Filtering~//
Proc. 26th Int. Conf. on World Wide Web (WWW). --- 2017. --- P.~173--182. --- URL: \url{https://doi.org/10.1145/3038912.3052569}.

\bibitem{sedhain2015autorec}
Sedhain~S., Menon~A.K., Sanner~S., Xie~L.
AutoRec: Autoencoders Meet Collaborative Filtering~//
Proc. 24th Int. Conf. on World Wide Web (WWW). --- 2015. --- P.~111--112. --- URL: \url{https://doi.org/10.1145/2740908.2742726}.

\bibitem{wang2019neural}
Wang~X., He~X., Wang~M., Feng~F., Chua~T.-S.
Neural Graph Collaborative Filtering~//
Proc. 42nd Int. ACM SIGIR Conf. --- 2019. --- P.~165--174. --- URL: \url{https://doi.org/10.1145/3331184.3331267}.

\bibitem{mcmahan2017communication}
McMahan~B., Moore~E., Ramage~D., Hampson~S., Ag\"uera y~Arcas~B.
Communication-Efficient Learning of Deep Networks from Decentralized Data~//
Proc. 20th Int. Conf. on Artificial Intelligence and Statistics (AISTATS). --- 2017. --- P.~1273--1282. --- URL: \url{https://arxiv.org/abs/1602.05629}.

\bibitem{li2020federated}
Li~T., Sahu~A.K., Zaheer~M., Sanjabi~M., Talwalkar~A., Smith~V.
Federated Optimization in Heterogeneous Networks~//
Proc. Machine Learning and Systems (MLSys). --- 2020. --- Vol.~2. --- P.~429--450. --- URL: \url{https://arxiv.org/abs/1812.06127}.

\bibitem{karimireddy2020scaffold}
Karimireddy~S.P., Kale~S., Mohri~M., Reddi~S.J., Stich~S.U., Suresh~A.T.
SCAFFOLD: Stochastic Controlled Averaging for Federated Learning~//
Proc. 37th Int. Conf. on Machine Learning (ICML). --- 2020. --- P.~5132--5143. --- URL: \url{https://arxiv.org/abs/1910.06378}.

\bibitem{wang2020tackling}
Wang~J., Liu~Q., Liang~H., Joshi~G., Poor~H.V.
Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization~//
Advances in Neural Information Processing Systems (NeurIPS). --- 2020. --- Vol.~33. --- URL: \url{https://arxiv.org/abs/2007.07481}.

\bibitem{ammad2019federated}
Ammad-ud-din~M., Ivannikova~E., Khan~S.A. et~al.
Federated Collaborative Filtering for Privacy-Preserving Personalized Recommendation System~//
arXiv preprint arXiv:1901.09888. --- 2019. --- URL: \url{https://arxiv.org/abs/1901.09888}.

\bibitem{chai2020secure}
Chai~D., Wang~L., Chen~K., Yang~Q.
Secure Federated Matrix Factorization~//
IEEE Intelligent Systems. --- 2021. --- Vol.~36, No.~5. --- P.~11--20. --- URL: \url{https://doi.org/10.1109/MIS.2020.3014880}.

\bibitem{lin2020fedrec}
Lin~G., Liang~F., Pan~W., Ming~Z.
FedRec: Federated Recommendation with Explicit Feedback~//
IEEE Intelligent Systems. --- 2021. --- Vol.~36, No.~5. --- P.~21--30. --- URL: \url{https://doi.org/10.1109/MIS.2020.3017205}.

\bibitem{muhammad2020fedfast}
Muhammad~K., Wang~Q., O'Reilly-Morgan~D., Tragos~E., Smyth~B., Hurley~N., Geraci~J., Lawlor~A.
FedFast: Going Beyond Average for Faster Training of Federated Recommender Systems~//
Proc. 26th ACM SIGKDD Int. Conf. --- 2020. --- P.~1234--1242. --- URL: \url{https://doi.org/10.1145/3394486.3403176}.

\bibitem{fallah2020personalized}
Fallah~A., Mokhtari~A., Ozdaglar~A.
Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach~//
Advances in Neural Information Processing Systems (NeurIPS). --- 2020. --- Vol.~33. --- URL: \url{https://arxiv.org/abs/2002.07948}.

\bibitem{harper2015movielens}
Harper~F.M., Konstan~J.A.
The MovieLens Datasets: History and Context~//
ACM Trans. on Interactive Intelligent Systems. --- 2015. --- Vol.~5, No.~4. --- Article~19. --- URL: \url{https://doi.org/10.1145/2827872}.

\bibitem{paszke2019pytorch}
Paszke~A., Gross~S., Massa~F. et~al.
PyTorch: An Imperative Style, High-Performance Deep Learning Library~//
Advances in Neural Information Processing Systems (NeurIPS). --- 2019. --- Vol.~32. --- P.~8026--8037. --- URL: \url{https://arxiv.org/abs/1912.01703}.

\bibitem{beutel2020flower}
Beutel~D.J., Topal~T., Mathur~A. et~al.
Flower: A Friendly Federated Learning Framework~//
arXiv preprint arXiv:2007.14390. --- 2020. --- URL: \url{https://arxiv.org/abs/2007.14390}.

\bibitem{kingma2015adam}
Kingma~D.P., Ba~J.
Adam: A Method for Stochastic Optimization~//
Proc. 3rd Int. Conf. on Learning Representations (ICLR). --- 2015. --- URL: \url{https://arxiv.org/abs/1412.6980}.

\bibitem{he2020lightgcn}
He~X., Deng~K., Wang~X., Li~Y., Zhang~Y., Wang~M.
LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation~//
Proc. 43rd Int. ACM SIGIR Conf. --- 2020. --- P.~639--648. --- URL: \url{https://doi.org/10.1145/3397271.3401063}.

\bibitem{kang2018selfattentive}
Kang~W.-C., McAuley~J.
Self-Attentive Sequential Recommendation~//
Proc. IEEE Int. Conf. on Data Mining (ICDM). --- 2018. --- P.~197--206. --- URL: \url{https://doi.org/10.1109/ICDM.2018.00035}.

\bibitem{sun2019bert4rec}
Sun~F., Liu~J., Wu~J., Pei~C., Lin~X., Ou~W., Jiang~P.
BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformers~//
Proc. 28th ACM Int. Conf. on Information and Knowledge Management (CIKM). --- 2019. --- P.~1441--1450. --- URL: \url{https://doi.org/10.1145/3357384.3357895}.

\bibitem{vorontsov2020ml}
Воронцов~К.В.
Математические методы обучения по прецедентам (теория обучения машин) [Электронный ресурс]: курс лекций.
--- М.: МФТИ, 2020. --- 263~с. --- URL: \url{http://www.machinelearning.ru/wiki/images/6/6d/Voron-ML-1.pdf}.

\bibitem{goodfellow2016deep}
Goodfellow~I., Bengio~Y., Courville~A.
Deep Learning.
--- MIT Press, 2016. --- 800~p. --- URL: \url{https://www.deeplearningbook.org/}.

\bibitem{rendle2020ncf}
Rendle~S., Krichene~W., Zhang~L., Anderson~J.
Neural Collaborative Filtering vs. Matrix Factorization Revisited~//
arXiv preprint arXiv:2005.09683. --- 2020. --- URL: \url{https://arxiv.org/abs/2005.09683}.

\end{thebibliography}

\newpage

\section*{Исходный код}

Исходный код проекта (модели, скрипты, конфигурация, отчёт) доступен в репозитории: \url{https://github.com/avanturer/fed_recsys}

\end{document}
